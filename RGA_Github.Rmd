---
title: "Web Analytics with R"
author: "Alexandros Papageorgiou"
date: "24th Sept, 2015"
output: ioslides_presentation
subtitle: DublinR, September 2015
---

## About the talk

* Intro Analytics

* Live Demo

* Practical Applications x 3

* Discussion

## About me

* Started @ Google Ireland - online sales & ops

* Career break (back to school)

* Web analyst @ WhatClinic.com

	
 
# Part I: Intro


## Getting started overview

1. Get some web data for a start

2. Get the right / acurate / relevant data `***`

3. Analyse the data


## Google Analytics API  + R 

**Why ?**


* Freedom from the limits of the GA user interface
* Automation, reproducibility, applications
* Richer datasets up to 7 Dimensions and 10 Metrics


**Large queries ?**


* Handle queries of 10K - 1M records 
* Mitigate the effect of Query Sampling 


## The package: RGA

```{r eval=F } 
install("RGA") 
```


* Author Artem Klevtsov

* Access to multiple GA APIs 

* Shiny app to explore dimensions and metrics.

* Actively developped + good documentation


# Part II: Demo

# Part III: Applications

## Practical applications

**Ecommerce website (simulated data)**


* Advertising campaign effectiveness (Key Ratios)

* Adgroup performance (Clustering)

* Key factors leading to conversion (Decision Tree)

## Libraries 

```{r message = FALSE, warning= FALSE}
library(RGA)
library(dplyr)
library(tidyr)
library(ggplot2)
```

## 1. Key Performance Ratios

* Commonly used in Business and finance analysis
* Good for data exploration in context 

<center><img src="D:/DublinR/Images/ratio3.jpg"
	height="250px"/></center>

## Key Ratios: Getting the data


```{r, eval=FALSE}
by_medium <- get_ga(profile.id = 106368203,
                    start.date = "2015-11-01", 
                    end.date = "2015-08-21", 
                           
                    metrics = "ga:transactions, ga:sessions",
                    dimensions = "ga:date, ga:medium",
                           
                    sort = NULL, 
                    filters = NULL, 
                    segment = NULL, 
                           
                    sampling.level = NULL,
                    start.index = NULL, 
                    max.results = NULL)
```



## Sessions and Transactions by medium


```{r  cache=F}
head(by_medium)
```


## Calculating the ratios


(Mathjax equation which does not render on Github)

\(ConversionQualityIndex = {\% Transactions/Medium\over \% Sessions/Medium}\)

```{r results='hide', cache=F}
by_medium_ratios <- by_medium  %>% 
    
    group_by(date) %>%  # sum sessions & transactions by date
    
    mutate(tot.sess = sum(sessions), tot.trans = sum(transactions)) %>% 
    
    mutate(pct.sessions = 100*sessions/tot.sess,   # calculate % sessions by medium
           pct.trans = 100*transactions/tot.trans, # calculate % transactions by medium
           conv.rate = 100*transactions/sessions) %>%     # conversion rate by medium
    
    mutate(ConvQualityIndex = pct.trans/pct.sessions) %>%  # conv quality index.
    
    filter(medium %in% c("search", "display", "referral"))    # the top 3 channels
```

## Ratios table

```{r }
columns <- c(1, 2, 7:10)
head(by_medium_ratios[columns])  # display selected columns
```




## Sessions % by medium
```{r include= F}
library(ggplot2)
```
```{r cache= F, message= F, warning= F, fig.width= 9, fig.height=4}
ggplot(by_medium_ratios, aes(date, pct.sessions, color = medium)) + 
    geom_point() + geom_jitter()+ geom_smooth() + ylim(0, 100)
```


## Transactions % by medium
```{r cache= F, message= F, warning= F, fig.width= 9, fig.height=4}
ggplot(by_medium_ratios, aes(date, pct.trans, color = medium)) + 
    geom_point() + geom_jitter() + geom_smooth()  
```



## Conversion Quality Index by medium

```{r cache= F, message= F, warning= F, fig.width= 9, fig.height=4}
ggplot(by_medium_ratios, aes(date, ConvQualityIndex , color = medium)) + 
    geom_point(aes(size=tot.trans)) + geom_jitter() + geom_smooth() + ylim(0,  5) +
    geom_hline(yintercept = 1, linetype="dashed", size = 1, color = "white") 
```

## 2. Clustering for Ad groups

* Unsupervised learning 
* Discovers structure in data
* Based on a similarity criterion
* Applications in Marketing


<center><img src="D:/DublinR/Images/clustering.jpg"
	height="300px"/></center>

	
## Ad Group Clustering: Getting the Data



```{r eval=FALSE}  
 
profile.id = "12345678"

start.date = "2015-01-01"
end.date = "2015-03-31"

metrics = "ga:sessions, ga:transactions, 
           ga:adCost, ga:transactionRevenue, 
           ga:pageviewsPerSession"

dimensions = "ga:adGroup"

adgroup_data <-  get_ga(profile.id = profile.id, 
                    start.date = start.date, 
                    end.date = end.date,
                    metrics = metrics, 
                    dimensions = dimensions)
```        

        

## Hierarchical Clustering
```{r cache= F, message= F, warning= F, fig.width= 10, fig.height=4}
top_adgroups <- adgroup_data %>% 
    filter(transactions >10)  %>%    
    filter(ad.group!="(not set)")    

n <-  nrow(top_adgroups)

rownames(top_adgroups) <-  paste("adG", 1:n) # short codes for adgroups

top_adgroups <-  select(top_adgroups, -ad.group) # remove long adgroup names 

scaled_adgroups <- scale(top_adgroups)  # scale the values
```

## Matrix: Scaled adgroup values.
```{r, echo=F,   cache=F}
head(scaled_adgroups)
```

## Dendrogram
```{r cache= F, message= F, warning= F, fig.width= 9, fig.height=3}
hc <-  hclust(dist(scaled_adgroups) ) 

plot(hc, hang = -1)

rect.hclust(hc, k=3, border="red")   
```


## Heatmap.2
```{r cache= F, message= F, warning= F, evaluate = F}
library(gplots); library(RColorBrewer)
my_palette <- colorRampPalette(c('white', 'yellow', 'green'))(256)
heatmap.2(scaled_adgroups, 
          cexRow = 0.7, 
          cexCol = 0.7,          
          col = my_palette,     
          rowsep = c(1, 5, 10, 14),
          lwid = c(lcm(8),lcm(8)),
          srtCol = 45,
          adjCol = c(1, 1),
          colsep = c(1, 2, 3, 4),
          sepcolor = "white", 
          sepwidth = c(0.01, 0.01),  
          scale = "none",         
          dendrogram = "row",    
          offsetRow = 0,
          offsetCol = 0,
          trace="none") 
```	



## 3. Decision Trees

* Handle categorical + numerical variables
* Mimic human decion making process
* Greedy approach

<center><img src="D:/DublinR/Images/tree.png"
	height="300px"/></center>

## 3. Pushing the API
```{r eval=FALSE}  

profile.id = "12345678"
start.date = "2015-03-01"
end.date = "2015-03-31"

dimensions = "ga:dateHour, ga:minute, ga:sourceMedium, ga:operatingSystem, 
              ga:subContinent, ga:pageDepth, ga:daysSinceLastSession"

metrics = "ga:sessions, ga:percentNewSessions,  ga:transactions, 
           ga:transactionRevenue, ga:bounceRate, ga:avgSessionDuration,
           ga:pageviewsPerSession, ga:bounces, ga:hits"


segment_returning = "sessions::condition::ga:userType==Returning Visitor"
segment_new = "sessions::condition::ga:userType==New Visitor"

# additional dimension by merging new and returning visitor segment

ga_transReturnSub <-  get_ga(profile.id = profile.id, start.date = start.date, 
                             end.date = end.date, 
                             metrics = metrics, dimensions = dimensions, 
                             segment = segment_returning)

ga_transReturnSub <- addCol(ga_transReturnSub, Visitor = "return")

ga_transNewSub <- get_ga(profile.id = profile.id, start.date = start.date,
                         end.date = end.date, metrics = metrics,
                         dimensions = dimensions, segment = segment_new)

ga_transNewSub <- addCol(ga_transNewSub, Visitor = "new")

join_new_ret <- bind_rows(ga_transNewSub, ga_transReturnSub)

ga_data <-  join_new_ret

```



##  The Data



```{r, echo=F,   cache=T}
head(ga_data) 
```

## Imbalanced class

<right><img src="D:/DublinR/Images/needle.jpg"
	height="300px"/></right>

**Approach: Page depth>5 set as proxy to conversion**


## Data preparation

* Session data made granular 
* Removed invalid sessions 
* Extra dimension added (user type)
* Removed highly correlated vars
* Data split into train and test 
* Day of the week extracted from date
* Days since last session placed in buckets 
* Date converted into weekday or weekend
* Datehour split in two component variables
* Georgraphy split between top sub-continents and Other
* Hour converted to AM or PM



## Decision Tree with rpart 

```{r include=F, cache=T}

library(lubridate)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)

## starting data manipulation  -------------------------


funDayname <- function (x) {
    wday(parse_date_time(x, "%Y%m%d"), label = T)
}


# assumption that only 1 session is possible per time slice  >> keep obs where session equals one or transaction eq one (this leaves out some legit entries where session are genuinely gt 1)

data_no_multi <- ga_data %>% filter (sessions==1  |  transactions==1)


countries <- c("Southern Africa", "Southern Asia", 
               "Western Africa", "Western Asia", "Polynesia", "Melanesia", "(not set)", "South America",
               "Middle Africa", "Caribbean", "Central America", "Central Asia", "Eastern Africa", 
               "Southeast Asia", "Northern Africa" )

hours <- c("00" , "01" , "02" , "03" , "04" , "05" , "06", "07" , "08" , "09" , "10" , "11" , "12"  )

data_no_multi$Visitor <- as.character(data_no_multi$Visitor)


data_clean <-  data_no_multi %>% 
    
    select (-c(minute, sessions, transactionRevenue, bounceRate, percentNewSessions, avgSessionDuration,
               pageviewsPerSession, hits)) %>%
    
    separate (sourceMedium, c("source", "medium"), sep=" / ") %>% 
    
    select (-source) %>%
    
    separate (dateHour, c("ymd", "hour"), sep=-3) %>%
    
    mutate (day = funDayname(ymd))  %>% 
    
    select (-ymd) %>% 
    
    mutate (day= ifelse(day  %in% c("Sat", "Sun"), "Weekend", "Week" )) %>% 
    
    mutate (subContinent= ifelse(subContinent  %in% countries, "isOther", as.character(subContinent) )) %>%
    
    mutate (hour= ifelse(hour  %in% hours, "AM" , "PM"))    

# converting to factors or numeric ------------------

data_clean$day <- as.factor(data_clean$day)

data_clean$hour <- as.factor(data_clean$hour)

data_clean$medium <- as.factor(data_clean$medium)

data_clean$operatingSystem <- as.factor(data_clean$operatingSystem)

data_clean$subContinent <- as.factor(data_clean$subContinent)

data_clean$Visitor <- as.factor(data_clean$Visitor)

data_clean$pageDepth <- as.numeric(data_clean$pageDepth)

data_clean$daysSinceLastSession <- as.numeric(data_clean$daysSinceLastSession)

data_clean$transactions <- as.factor(data_clean$transactions)

## transforming the factor into bins --------------------------------------

levels <- c(-Inf, 0, 1, 2, 7, 30, Inf)
labels <- c("0", "1", "2", "2-7", "7-30", "30+")


## Continue and finalize data manipulation -----------

data_clean <- data_clean  %>% 
    mutate(daysSinceLastSession = cut(daysSinceLastSession, levels, labels = labels)) 


## Fit to rpart with pagedepth over 5 as target  ----

library(caTools)
library (ROCR)

set.seed(1)

data_pdepth <- data_clean %>%  filter (pageDepth!=0 & pageDepth!=1) %>%   
    mutate (pageDepth= ifelse(pageDepth <=4, "0", "1")) %>%  
    select (-transactions)

data_pdepth$pageDepth <- as.factor(data_pdepth$pageDepth)

split <- sample.split(data_pdepth$pageDepth, SplitRatio = 0.8)
Train <- subset(data_pdepth, split==TRUE)
Test <- subset(data_pdepth, split==FALSE)
```



```{r  cache=F, warning = F, message = F}

library(rpart)

fit <- rpart(pageDepth ~., data = Train,       # pageDepth is a binary variable
                            method = 'class',  
                            control=rpart.control(minsplit = 10, cp = 0.001, xval = 10)) 
# printcp(fit)

fit <- prune(fit, cp = 1.7083e-03)   # prune the tree based on chosen param value

```




## The Tree
```{r warning=F, message=F, echo=F, fig.width= 10, fig.height=5.5}
library(rattle)
fancyRpartPlot(fit, sub = NULL)
```

## VarImp
```{r echo=F}
fit$variable.importance <- fit$variable.importance[5:1]
```
```{r fig.width= 7, fig.height=3.5}
dotchart(fit$variable.importance)
```

## Takeaways

- Web analytics not just for marketers!

- But neither a magic bullet

- Solutions ? 

- What's coming next ?
















